import ModelNN as roc
from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score, roc_auc_score, auc, roc_curve
import matplotlib.pyplot as plt

false_positive_rate, recall, thresholds = roc_curve(roc.y_test0,roc.y_pred0)
roc_auc = auc(false_positive_rate, recall)
print(roc_auc)
print(false_positive_rate, recall, thresholds)
plt.figure()
plt.title('ROC Strength 0')
plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1], [0,1], 'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('Recall')
plt.xlabel('Fall-out (1-Specificity)')

false_positive_rate, recall, thresholds = roc_curve(roc.y_test1,roc.y_pred1)
roc_auc = auc(false_positive_rate, recall)
print(roc_auc)
print(false_positive_rate, recall, thresholds)
plt.figure()
plt.title('ROC Strength 1')
plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1], [0,1], 'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('Recall')
plt.xlabel('Fall-out (1-Specificity)')

false_positive_rate, recall, thresholds = roc_curve(roc.y_test2,roc.y_pred2)
roc_auc = auc(false_positive_rate, recall)
print(roc_auc)
print(false_positive_rate, recall, thresholds)
plt.figure()
plt.title('ROC Strength 2')
plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1], [0,1], 'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('Recall')
plt.xlabel('Fall-out (1-Specificity)')
plt.show()   